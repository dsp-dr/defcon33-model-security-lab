#!/usr/bin/env python3
"""
Experiment 002: PyTorch JIT Model Security Analyzer
Analyzes TorchScript models for potential security issues
"""

import torch
import zipfile
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any

class JITAnalyzer:
    """Analyzes PyTorch JIT models for security vulnerabilities"""
    
    def __init__(self):
        self.results = {
            'models_analyzed': 0,
            'suspicious_models': [],
            'clean_models': [],
            'findings': []
        }
    
    def analyze_torchscript_zip(self, model_path: str) -> Dict[str, Any]:
        """Analyze the internal structure of a TorchScript model"""
        
        print(f"\nðŸ” Analyzing TorchScript model: {model_path}")
        findings = {
            'path': model_path,
            'is_zip': False,
            'contains_code': False,
            'contains_pickle': False,
            'suspicious_files': [],
            'metadata': {}
        }
        
        try:
            # Check if it's a zip file (TorchScript format)
            if zipfile.is_zipfile(model_path):
                findings['is_zip'] = True
                print("  âœ“ Model is in ZIP format (TorchScript)")
                
                with zipfile.ZipFile(model_path, 'r') as zf:
                    file_list = zf.namelist()
                    
                    # Check for suspicious files
                    for fname in file_list:
                        # Check for code files
                        if fname.endswith('.py'):
                            findings['contains_code'] = True
                            findings['suspicious_files'].append(fname)
                            print(f"  âš ï¸  Found Python code: {fname}")
                            
                            # Extract and analyze code
                            try:
                                code_content = zf.read(fname).decode('utf-8')
                                self._analyze_code(code_content, fname, findings)
                            except:
                                pass
                        
                        # Check for pickle data
                        if 'data.pkl' in fname or fname.endswith('.pkl'):
                            findings['contains_pickle'] = True
                            print(f"  âš ï¸  Found pickle data: {fname}")
                        
                        # Extract metadata
                        if fname == 'constants.pkl':
                            try:
                                constants_data = zf.read(fname)
                                # Note: Not actually unpickling for safety
                                findings['metadata']['has_constants'] = True
                            except:
                                pass
            else:
                print("  â„¹ï¸  Not a ZIP file, likely a pickle-based model")
                findings['contains_pickle'] = True
                
        except Exception as e:
            print(f"  âŒ Error analyzing model: {e}")
            findings['error'] = str(e)
        
        self.results['models_analyzed'] += 1
        
        # Determine if suspicious
        if findings['contains_code'] or findings['suspicious_files']:
            self.results['suspicious_models'].append(model_path)
            findings['risk_level'] = 'HIGH'
        elif findings['contains_pickle']:
            findings['risk_level'] = 'MEDIUM'
        else:
            self.results['clean_models'].append(model_path)
            findings['risk_level'] = 'LOW'
        
        self.results['findings'].append(findings)
        return findings
    
    def _analyze_code(self, code: str, filename: str, findings: Dict):
        """Analyze embedded Python code for suspicious patterns"""
        
        suspicious_patterns = [
            ('import os', 'System access'),
            ('import subprocess', 'Process execution'),
            ('import socket', 'Network access'),
            ('eval(', 'Dynamic code execution'),
            ('exec(', 'Dynamic code execution'),
            ('__import__', 'Dynamic imports'),
            ('open(', 'File system access'),
            ('compile(', 'Code compilation')
        ]
        
        for pattern, description in suspicious_patterns:
            if pattern in code:
                warning = f"{filename}: {description} ({pattern})"
                findings.setdefault('code_warnings', []).append(warning)
                print(f"    ðŸš¨ {warning}")
    
    def safe_load_metadata(self, model_path: str) -> Dict:
        """Safely extract model metadata without loading the model"""
        
        metadata = {
            'file_size': os.path.getsize(model_path),
            'file_type': 'unknown',
            'pytorch_version': None
        }
        
        try:
            if zipfile.is_zipfile(model_path):
                metadata['file_type'] = 'torchscript_zip'
                
                with zipfile.ZipFile(model_path, 'r') as zf:
                    # Try to read version info
                    if 'version' in zf.namelist():
                        version = zf.read('version').decode('utf-8').strip()
                        metadata['pytorch_version'] = version
                    
                    # Get file structure
                    metadata['internal_files'] = zf.namelist()
            else:
                metadata['file_type'] = 'pickle_based'
                
        except Exception as e:
            metadata['error'] = str(e)
        
        return metadata
    
    def generate_report(self) -> str:
        """Generate a security analysis report"""
        
        report = []
        report.append("\n" + "="*60)
        report.append("ðŸ“Š JIT Model Security Analysis Report")
        report.append("="*60)
        report.append(f"Models analyzed: {self.results['models_analyzed']}")
        report.append(f"Suspicious models: {len(self.results['suspicious_models'])}")
        report.append(f"Clean models: {len(self.results['clean_models'])}")
        
        if self.results['suspicious_models']:
            report.append("\nâš ï¸  Suspicious Models:")
            for model in self.results['suspicious_models']:
                report.append(f"  â€¢ {model}")
                # Find and display warnings
                for finding in self.results['findings']:
                    if finding['path'] == model:
                        if 'code_warnings' in finding:
                            for warning in finding['code_warnings']:
                                report.append(f"    - {warning}")
        
        report.append("\nðŸ”’ Recommendations:")
        report.append("  1. Use torch.jit.load with strict=False to disable code execution")
        report.append("  2. Load models in sandboxed environments")
        report.append("  3. Verify model checksums before loading")
        report.append("  4. Use traced models instead of scripted when possible")
        report.append("  5. Scan model files before deployment")
        
        return "\n".join(report)

def create_test_models():
    """Create test JIT models for analysis"""
    
    os.makedirs("test_jit_models", exist_ok=True)
    
    # Create a simple traced model (safer)
    class SimpleModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = torch.nn.Linear(10, 5)
        
        def forward(self, x):
            return self.linear(x)
    
    print("Creating test JIT models...")
    
    # Traced model (safer)
    model = SimpleModel()
    example_input = torch.randn(1, 10)
    traced_model = torch.jit.trace(model, example_input)
    traced_model.save("test_jit_models/traced_model.pt")
    print("  âœ“ Created traced_model.pt (safe)")
    
    # Scripted model (can contain arbitrary code)
    @torch.jit.script
    def scripted_function(x: torch.Tensor) -> torch.Tensor:
        # This could potentially contain malicious code
        return x * 2
    
    torch.jit.save(scripted_function, "test_jit_models/scripted_function.pt")
    print("  âœ“ Created scripted_function.pt")
    
    return ["test_jit_models/traced_model.pt", "test_jit_models/scripted_function.pt"]

if __name__ == "__main__":
    analyzer = JITAnalyzer()
    
    print("ðŸ”¬ PyTorch JIT Security Analyzer")
    print("="*60)
    
    # Create test models
    test_models = create_test_models()
    
    # Analyze test models
    for model_path in test_models:
        analyzer.analyze_torchscript_zip(model_path)
        metadata = analyzer.safe_load_metadata(model_path)
        print(f"  Metadata: {metadata['file_type']}, PyTorch {metadata.get('pytorch_version', 'unknown')}")
    
    # Analyze command line arguments
    if len(sys.argv) > 1:
        for model_path in sys.argv[1:]:
            if os.path.exists(model_path):
                analyzer.analyze_torchscript_zip(model_path)
    
    # Generate and print report
    print(analyzer.generate_report())
    
    # Save results
    with open("jit_analysis_results.json", "w") as f:
        json.dump(analyzer.results, f, indent=2)
    print("\nâœ“ Results saved to jit_analysis_results.json")