#+TITLE: DEF CON 33 AI/Security Analysis Pipeline
#+AUTHOR: Security Research
#+DATE: 2025-08-07
#+PROPERTY: header-args :mkdirp yes

* Overview
Process and analyze DEF CON 33 presentations focusing on AI/ML security implications.

* Setup Environment
#+BEGIN_SRC bash :tangle setup.sh
#!/bin/bash
# Create project structure
mkdir -p {analysis,extracted,models,src}

# Install dependencies
pip install torch numpy safetensors pdf2image pytesseract
pip install transformers # for text analysis
#+END_SRC

* PDF Processing Pipeline

** Extract Text from PDFs
#+BEGIN_SRC python :tangle src/extract_pdf_text.py
#!/usr/bin/env python3
"""Extract and analyze text from DEF CON PDFs"""

import subprocess
import json
from pathlib import Path
import re

def extract_pdf_text(pdf_path):
    """Extract text using pdf_text"""
    try:
        result = subprocess.run(
            ['pdf_text', str(pdf_path)],
            capture_output=True,
            text=True
        )
        return result.stdout
    except Exception as e:
        print(f"Error extracting {pdf_path}: {e}")
        return ""

def categorize_presentation(filename, content):
    """Categorize presentation by security domain"""
    categories = {
        'ai_ml': ['model', 'AI', 'LLM', 'neural', 'GPT', 'machine learning', 'PyTorch'],
        'supply_chain': ['supply chain', 'dependency', 'npm', 'package', 'S3 bucket'],
        'hardware': ['hardware', 'firmware', 'embedded', 'DMA', 'modem'],
        'cloud': ['cloud', 'VPN', 'Azure', 'Entra', 'AWS'],
        'auth': ['authentication', 'FIDO', 'passkey', 'WebAuthn', 'SSO'],
        'malware': ['malware', 'rootkit', 'bootkit', 'C2', 'command control']
    }
    
    found_categories = []
    content_lower = content.lower()
    
    for category, keywords in categories.items():
        if any(keyword.lower() in content_lower for keyword in keywords):
            found_categories.append(category)
    
    return found_categories

def process_defcon_pdfs(pdf_dir):
    """Process all DEF CON PDFs"""
    pdf_dir = Path(pdf_dir)
    results = {}
    
    # Focus on AI/ML related presentations
    ai_ml_pdfs = [
        "Cyrus Parzian - Loading Models, Launching Shells Abusing AI File Formats for Code Execution.pdf",
        "Ji'an Zhou Lishuo Song - Safe Harbor or Hostile Waters Unveiling the Hidden Perils of the TorchScript Engine in PyTorch.pdf",
        "Ben Nassi Or Yair - Stav Cohen - Invitation Is All You Need Invoking Gemini for Workspace Agents with a Simple Google Calendar Invite.pdf"
    ]
    
    for pdf_file in pdf_dir.glob("*.pdf"):
        print(f"Processing: {pdf_file.name}")
        
        # Extract text
        text = extract_pdf_text(pdf_file)
        
        # Categorize
        categories = categorize_presentation(pdf_file.name, text)
        
        # Store results
        results[pdf_file.name] = {
            'categories': categories,
            'size_mb': pdf_file.stat().st_size / 1024 / 1024,
            'text_preview': text[:500] if text else "No text extracted",
            'is_ai_ml': 'ai_ml' in categories
        }
        
        # Save full text for AI/ML presentations
        if 'ai_ml' in categories:
            output_file = Path('extracted') / f"{pdf_file.stem}.txt"
            output_file.write_text(text)
    
    # Save analysis results
    with open('analysis/defcon33_analysis.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    return results

if __name__ == "__main__":
    results = process_defcon_pdfs("defcon33/.mirror/")
    
    # Print AI/ML focused presentations
    print("\nðŸ¤– AI/ML Security Presentations:")
    for name, data in results.items():
        if data['is_ai_ml']:
            print(f"  - {name} ({data['size_mb']:.1f} MB)")
#+END_SRC

** Analyze AI/ML Attack Vectors
#+BEGIN_SRC python :tangle src/analyze_ai_attacks.py
#!/usr/bin/env python3
"""Analyze AI/ML attack vectors from DEF CON presentations"""

import re
from pathlib import Path
from collections import defaultdict

def extract_attack_patterns(text):
    """Extract attack patterns and vulnerabilities"""
    patterns = {
        'serialization': [
            r'pickle.*vulnerabilit',
            r'deserialization.*attack',
            r'model.*file.*format',
            r'torch\.load.*unsafe'
        ],
        'supply_chain': [
            r'model.*repository',
            r'huggingface.*compromise',
            r'pretrained.*malicious',
            r'model.*zoo.*attack'
        ],
        'code_execution': [
            r'arbitrary.*code.*execution',
            r'RCE.*model',
            r'shell.*injection',
            r'command.*injection'
        ],
        'llm_specific': [
            r'prompt.*injection',
            r'jailbreak.*LLM',
            r'context.*manipulation',
            r'token.*poisoning'
        ]
    }
    
    found_attacks = defaultdict(list)
    
    for attack_type, regex_patterns in patterns.items():
        for pattern in regex_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                found_attacks[attack_type].extend(matches)
    
    return dict(found_attacks)

def create_attack_matrix():
    """Create MITRE ATT&CK style matrix for AI/ML"""
    matrix = {
        "Initial Access": [
            "Malicious Model Upload",
            "Supply Chain Compromise",
            "Model Repository Poisoning"
        ],
        "Execution": [
            "Pickle Deserialization",
            "TorchScript Exploitation",
            "ONNX Runtime Abuse"
        ],
        "Persistence": [
            "Model Checkpoint Backdoor",
            "Training Pipeline Injection",
            "Gradient Poisoning"
        ],
        "Defense Evasion": [
            "Model Obfuscation",
            "Adversarial Perturbations",
            "Steganographic Weights"
        ],
        "Exfiltration": [
            "Model Inversion",
            "Membership Inference",
            "Training Data Extraction"
        ]
    }
    return matrix

# Analyze extracted texts
for txt_file in Path('extracted').glob('*.txt'):
    print(f"\nAnalyzing: {txt_file.name}")
    text = txt_file.read_text()
    attacks = extract_attack_patterns(text)
    
    for attack_type, instances in attacks.items():
        print(f"  {attack_type}: {len(instances)} instances found")
#+END_SRC

* Model Security Testing Framework

** Safe Model Loading Tests
#+BEGIN_SRC python :tangle src/test_model_security.py :results output
#!/usr/bin/env python3
"""Test model loading security"""

import torch
import pickle
import tempfile
from pathlib import Path

def test_pickle_vulnerability():
    """Demonstrate pickle code execution risk"""
    print("ðŸ”¬ Testing Pickle Vulnerability")
    
    # Create a malicious "model"
    class EvilModel:
        def __reduce__(self):
            import os
            return (os.system, ('echo "Code executed during unpickling!"',))
    
    # Save it
    with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:
        pickle.dump(EvilModel(), f)
        evil_path = f.name
    
    print(f"  âœ“ Created malicious pickle at: {evil_path}")
    print("  âš ï¸  DO NOT LOAD THIS FILE")
    
    # Safe alternative
    print("\nâœ… Safe Loading Methods:")
    print("  1. torch.load(file, weights_only=True)")
    print("  2. Use safetensors format")
    print("  3. Use ONNX with verification")
    print("  4. Custom binary formats (GGML/GGUF)")

def test_model_formats():
    """Test different model serialization formats"""
    print("\nðŸ“Š Model Format Security Comparison")
    
    formats = {
        'pickle': {'safe': False, 'exec_risk': 'HIGH', 'use_case': 'Legacy only'},
        'torch': {'safe': False, 'exec_risk': 'HIGH', 'use_case': 'With weights_only=True'},
        'safetensors': {'safe': True, 'exec_risk': 'NONE', 'use_case': 'Recommended'},
        'onnx': {'safe': True, 'exec_risk': 'LOW', 'use_case': 'Cross-platform'},
        'ggml/gguf': {'safe': True, 'exec_risk': 'NONE', 'use_case': 'Inference optimized'}
    }
    
    for fmt, props in formats.items():
        print(f"\n  {fmt}:")
        print(f"    Safe: {'âœ…' if props['safe'] else 'âŒ'}")
        print(f"    Execution Risk: {props['exec_risk']}")
        print(f"    Use Case: {props['use_case']}")

if __name__ == "__main__":
    test_pickle_vulnerability()
    test_model_formats()
#+END_SRC

** Model Scanning Tool
#+BEGIN_SRC python :tangle src/scan_model.py
#!/usr/bin/env python3
"""Scan model files for potential security issues"""

import sys
import pickle
import struct
import zipfile
from pathlib import Path

class SafeUnpickler(pickle.Unpickler):
    """Restricted unpickler that blocks dangerous operations"""
    
    ALLOWED_MODULES = {
        'torch', 'torch.nn', 'torch.nn.modules',
        'numpy', 'collections', 'torch._utils'
    }
    
    def find_class(self, module, name):
        if module not in self.ALLOWED_MODULES:
            raise pickle.UnpicklingError(
                f"Blocked unsafe module: {module}.{name}"
            )
        return super().find_class(module, name)

def scan_model_file(filepath):
    """Scan a model file for security issues"""
    filepath = Path(filepath)
    print(f"\nðŸ” Scanning: {filepath.name}")
    
    issues = []
    
    # Check file type
    with open(filepath, 'rb') as f:
        header = f.read(16)
    
    # Check for pickle format
    if header.startswith(b'\x80'):  # Pickle protocol
        issues.append("âŒ File uses pickle format (code execution risk)")
        
        # Try restricted unpickling
        try:
            with open(filepath, 'rb') as f:
                SafeUnpickler(f).load()
            print("  âœ“ Passed restricted unpickle test")
        except pickle.UnpicklingError as e:
            issues.append(f"  ðŸš¨ Unsafe pickle content: {e}")
    
    # Check for PyTorch format
    elif filepath.suffix in ['.pt', '.pth']:
        # PyTorch files are zip archives
        try:
            with zipfile.ZipFile(filepath, 'r') as z:
                files = z.namelist()
                if 'data.pkl' in files:
                    issues.append("âš ï¸  PyTorch file contains pickle data")
                print(f"  ðŸ“¦ Archive contains: {files}")
        except:
            pass
    
    # Check for safe formats
    elif header.startswith(b'GGML') or header.startswith(b'GGUF'):
        print("  âœ… Safe GGML/GGUF format detected")
    
    elif b'safetensors' in header:
        print("  âœ… Safe safetensors format detected")
    
    # Report findings
    if issues:
        print("\nâš ï¸  Security Issues Found:")
        for issue in issues:
            print(f"  {issue}")
    else:
        print("  âœ… No immediate security issues detected")
    
    return issues

if __name__ == "__main__":
    if len(sys.argv) > 1:
        scan_model_file(sys.argv[1])
    else:
        print("Usage: python scan_model.py <model_file>")
#+END_SRC

* Mermaid Diagrams

** AI/ML Attack Flow
#+BEGIN_SRC mermaid :file diagrams/ai_attack_flow.png
graph TB
    A[Attacker] --> B[Create Malicious Model]
    B --> C{Distribution Vector}
    
    C -->|Model Hub| D[HuggingFace/ModelZoo]
    C -->|Direct Share| E[GitHub/Google Drive]
    C -->|Supply Chain| F[Dependency Injection]
    
    D --> G[Victim Downloads Model]
    E --> G
    F --> G
    
    G --> H[Load Model]
    
    H -->|pickle.load| I[Code Execution]
    H -->|torch.load| J[Potential RCE]
    H -->|safetensors| K[Safe Loading]
    
    I --> L[System Compromise]
    J --> L
    
    L --> M[Data Exfiltration]
    L --> N[Backdoor Installation]
    L --> O[Lateral Movement]
    
    style I fill:#ff6b6b
    style J fill:#ffd43b
    style K fill:#51cf66
    style L fill:#ff6b6b
#+END_SRC

** Model Format Security Comparison
#+BEGIN_SRC mermaid :file diagrams/format_security.png
graph LR
    subgraph "Unsafe Formats"
        A[.pkl Files] --> X[Code Execution]
        B[.pt/.pth Files] --> X
    end
    
    subgraph "Safe Formats"
        C[GGML/GGUF] --> Y[Data Only]
        D[SafeTensors] --> Y
        E[ONNX*] --> Y
    end
    
    subgraph "Mitigation"
        F[weights_only=True]
        G[Sandboxing]
        H[Hash Verification]
    end
    
    B -.->|with| F
    X -.->|prevent with| G
    Y -.->|verify with| H
    
    style A fill:#ff6b6b
    style B fill:#ffd43b
    style C fill:#51cf66
    style D fill:#51cf66
    style E fill:#51cf66
#+END_SRC

* Summary and Recommendations

** Key Findings from DEF CON 33
1. Model files are a significant attack vector
2. PyTorch's default serialization uses pickle (unsafe)
3. Supply chain attacks via model repositories are practical
4. New formats (GGUF, SafeTensors) designed for security

** Security Best Practices
1. *Never* load untrusted pickle files
2. Use ~weights_only=True~ with torch.load()
3. Prefer SafeTensors or GGML/GGUF formats
4. Verify model hashes before loading
5. Run model loading in sandboxed environments
6. Scan models with security tools before use

** Future Research Directions
- Automated model security scanning
- Cryptographic model signing
- Secure model distribution protocols
- Runtime model integrity verification
